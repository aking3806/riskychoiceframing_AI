{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6839655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82036988",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"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\" \n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cde3316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7dJkxjArqwIKvdnd9P5p1vsIvG6Nb at 0x7f8df0a13b80> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Remote Work:\\n\\nPros:\\n1. Flexibility: Remote work allows for flexible hours and the option to work in an environment that suits the individual's comfort and productivity levels.\\n2. Commute: No commuting saves time, money, and reduces stress.\\n3. Work-Life Balance: It easier to balance personal responsibilities with work\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1689605091,\n",
       "  \"id\": \"chatcmpl-7dJkxjArqwIKvdnd9P5p1vsIvG6Nb\",\n",
       "  \"model\": \"gpt-4-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 64,\n",
       "    \"prompt_tokens\": 20,\n",
       "    \"total_tokens\": 84\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###GPT4 \n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Analyze the pros and cons of remote work vs. office work\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.8,\n",
    "  max_tokens=64,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9cce6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def turbo_test(messages,\n",
    "               model=\"gpt-3.5-turbo\", \n",
    "               temperature=0, \n",
    "               max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}],\n",
    "        model=model,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens\n",
    ")\n",
    "    return response.choices[0].message['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e074d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "084271cc",
   "metadata": {},
   "source": [
    "The GPT-35-Turbo and GPT-4 models are optimized to work with inputs formatted as a conversation. \n",
    "\n",
    "The messages variable passes an array of dictionaries with different roles in the conversation delineated by \n",
    "system, user, and assistant. The system message can be used to prime the model by including context or \n",
    "instructions on how the model should respond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f7f41",
   "metadata": {},
   "source": [
    "- Be Specific. Leave as little to interpretation as possible. Restrict the operational space.\n",
    "- Be Descriptive. Use analogies.\n",
    "- Double Down. Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc.\n",
    "- Order Matters. The order in which you present information to the model may impact the output. Whether you put instructions before your content (“summarize the following…”) or after (“summarize the above…”) can make a difference in output. Even the order of few-shot examples can matter. This is referred to as recency bias.\n",
    "- Give the model an “out”. It can sometimes be helpful to give the model an alternative path if it is unable to complete the assigned task. For example, when asking a question over a piece of text you might include something like \"respond with ‘not found’ if the answer is not present.\" This can help the model avoid generating false responses.\n",
    "- Tables – As shown in the examples in the previous section, GPT models can understand tabular formatted data quite easily. This can be a space efficient way to include data, rather than preceding every field with name (such as with JSON).\n",
    "- White Space – Consecutive whitespaces are treated as separate tokens which can be an easy way to waste space. Spaces preceding a word, on the other hand, are typically treated as part of the same token as the word. Carefully watch your usage of whitespace and don’t use punctuation when a space alone will do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e22c29",
   "metadata": {},
   "source": [
    "#### System role\n",
    "The system role also known as the system message is included at the beginning of the array. This message provides the initial instructions to the model. You can provide various information in the system role including:\n",
    "\n",
    "- A brief description of the assistant\n",
    "- Personality traits of the assistant\n",
    "- Instructions or rules you would like the assistant to follow\n",
    "- Data or information needed for the model, such as relevant questions from an FAQ\n",
    "- You can customize the system role for your use case or just include basic instructions. The system role/message is optional, but it's recommended to at least include a basic one to get the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75839bdc",
   "metadata": {},
   "source": [
    "link: https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/2/guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12885024",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 20\u001b[0m\n\u001b[1;32m     11\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     12\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     13\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     14\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature, \n\u001b[1;32m     15\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m response \u001b[38;5;241m=\u001b[39m get_completion_from_messages(messages, \u001b[43mmodel\u001b[49m, temperature, max_tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "###example from DeepLearning AI \n",
    "\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
    "\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "response = get_completion_from_messages(messages, model, temperature, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ebb8c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"},\n",
       " {'role': 'user', 'content': \"What's the weather like today\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
    "#chat_response = chat_completion_request(\n",
    "  #  messages, functions=functions)\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
